[{"categories":null,"contents":"概要 高次元データの可視化によく用いられるt-SNEの動作を理解するために、論文を読んで実装してみることにしました。パラメータ更新時の勾配計算で楽をしたいのでPyTorchで実装します。\n基本的には元論文を参考に実装を行いました。\n今回使ったコードはGithubにあげています。\nt-SNE自体の解説記事ではありません。そのためt-SNEを既にある程度理解しているものと想定しています。\nSNE t-SNEには前身であるSNEなる手法が存在し、t-SNEはSNEの弱点を補った手法です。そこでまずはSNEを実装します。\n詳細は論文を参考して頂きたいのですが、大まかな流れは以下の通り。\n 入力データ $ X \\in R ^ {N \\times d} $ 指定するパラメータは主にn_componentsとperplexity。前者は圧縮したい次元数で後者は後ほど説明。 出力は低次元に圧縮されたデータ $ y \\in R ^ {N \\times ncomponents} $    $ y $ をランダムに初期化\n  高次元空間の各データポイントに対応する正規分布の分散を指定されたperplexityから求める。\n  高次元空間における各データポイント間の類似度を求める。\n  収束するまで以下を繰り返し\n 低次元空間における各データポイント間の類似度を求める。 高次元空間と低次元空間における類似度の分布が近づく方向へ $ y $ を更新    perplexityですが、高次元における各データポイントの類似度 $ p_{j|i} $ を自分以外の全ての $ j $ について求めたもののシャノンエントロピーとして定義されています。\nつまりは、SNEは高次元の類似度を低次元でも保つように低次元表現を学習しますが、その高次元の類似度を算出する際に各データポイントの近傍をどれくらいまで考慮するのか、ということを調節していると考えられます。\n極端に考えれば、perplexityをめちゃくちゃ小さくすると各データポイントの類似度のエントロピーが小さいことを意味するので、対応する正規分布の分散は小さいものに設定している、つまり本当に近傍にあるデータポイントのみを考慮して類似度を算出していると考えられます。\n詳しくは論文を参照ください。\n注意点   論文では、正規分布の分散を二部探索で求めているとの記述がありましたが、以下の実装では単に範囲を決め打ちして最も指定のperplexityに近いものを選んでいます。","date":"2020-12-17T00:00:00Z","permalink":"https://ykskks.github.io/blog/tsne-pytorch/","tags":["PyTorch","機械学習"],"title":"PyTorchでt-SNEを実装"},{"categories":null,"contents":"概要 先日までkaggleのCommonLit Readability Prizeに参加しており、若干のshakedownをしてしまったものの最終的に132位で銀メダルを獲得することができました。\n簡単にいうと、本コンペの目的は与えられた文章の「読みやすさ(Readability)」を予測することです。この指標は絶対的に定義できるものではないので、人間（現役教師）に2つの文章の比較を行ってもらった結果を元にこちらのDiscussionのような方法で計算されています。\nコンペの詳細はリンクを参照していただくとして、ここでは自分の取り組みと上位陣のSolutionをまとめたいと思います。\n自分の解法 CV KFold\nモデル 以下のモデルたちのNetflix Blending\n RoBERTa-large (22, 23, 24層のhidden statesのmeanをとってconcat → Linear) RoBERTa-large (12, 18, 24層のhidden statesのmeanをとってconcat → Linear) RoBERTa-large (7, 15, 23層のhidden statesのmeanをとってconcat → Linear) RoBERTa-large (1~24層のhidden statesのmeanをとってattention pooling → Linear) RoBERTa-large (2, 4, \u0026hellip; , 22, 24層のhidden statesのmeanをとってattention pooling → Linear) RoBERTa-base (コンペデータでpretrain、10, 11, 12層のhidden statesのmeanをとってconcat → Linear) Pre-trained Roberta solution in PyTorch (公開Notebook) CommonLit Readability Prize-RoBERTa Torch|Infer 3 (公開Notebook)  Public LBへのoverfitを避けるためNetflix blendingの正則化は大きめに設定した（0.","date":null,"permalink":"https://ykskks.github.io/blog/commonlit/","tags":["kaggle","自然言語処理"],"title":"kaggle - CommonLit Readability Prize 参加記"}]