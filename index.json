[{"categories":null,"contents":"概要 高次元データの可視化によく用いられるt-SNEの動作を理解するために、論文を読んで実装してみることにしました。パラメータ更新時の勾配計算で楽をしたいのでPyTorchで実装します。\n基本的には元論文を参考に実装を行いました。\n今回使ったコードはGithubにあげています。\nt-SNE自体の解説記事ではありません。そのためt-SNEを既にある程度理解しているものと想定しています。\nSNE t-SNEには前身であるSNEなる手法が存在し、t-SNEはSNEの弱点を補った手法です。そこでまずはSNEを実装します。\n詳細は論文を参考して頂きたいのですが、大まかな流れは以下の通り。\n 入力データ $ X $ (N x d行列) 指定するパラメータは主にn_componentsとperplexity。前者は圧縮したい次元数で後者は後ほど説明。 出力は低次元に圧縮されたデータ $ y $ (N x n_components行列)    $ y $ をランダムに初期化\n  高次元空間の各データポイントに対応する正規分布の分散を指定されたperplexityから求める。\n  高次元空間における各データポイント間の類似度を求める。\n  収束するまで以下を繰り返し\n 低次元空間における各データポイント間の類似度を求める。 高次元空間と低次元空間における類似度の分布が近づく方向へ $ y $ を更新    perplexityですが、高次元における各データポイントの類似度 $ p_{j|i} $ を自分以外の全ての $ j $ について求めたもののシャノンエントロピーとして定義されています。\nつまりは、SNEは高次元の類似度を低次元でも保つように低次元表現を学習しますが、その高次元の類似度を算出する際に各データポイントの近傍をどれくらいまで考慮するのか、ということを調節していると考えられます。\n極端に考えれば、perplexityをめちゃくちゃ小さくすると各データポイントの類似度のエントロピーが小さいことを意味するので、対応する正規分布の分散は小さいものに設定している、つまり本当に近傍にあるデータポイントのみを考慮して類似度を算出していると考えられます。\n詳しくは論文を参照ください。\n注意点   論文では、正規分布の分散を二部探索で求めているとの記述がありましたが、以下の実装では単に範囲を決め打ちして最も指定のperplexityに近いものを選んでいます。\n  引数のmodeは高次元での計算であるか、低次元での計算であるかのフラグです。SNEでは影響がないのですが、後ほどのt-SNEの実装のためにあえて残しています。\n  正直あまり効率のいいコードではないかもしれません。何かあれば是非ご指摘ください。","date":"2021-07-15T00:00:00Z","permalink":"https://ykskks.github.io/blog/tsne-pytorch/","tags":["PyTorch","機械学習"],"title":"PyTorchでt-SNEを実装"}]